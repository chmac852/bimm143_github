---
title: "class08 Lab"
format: gfm
---

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
```
```{r}
head(wisc.df)
```
```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
```
```{r}
# Create diagnosis vector for later 
diagnosis <- factor(wisc.df$diagnosis) 
```

Q1. How many observations are in this dataset?
569
```{r}
nrow(wisc.data)
dim(wisc.data)
```

Q2. How many of the observations have a malignant diagnosis?
212

```{r}
length(grep("M", diagnosis))
```

Q3. How many variables/features in the data are suffized with _mean?
10
```{r}
meancol <- colnames(wisc.data)[grep("_mean", colnames(wisc.data))]
length(meancol)
```
```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```
```{r}
wisc.pr <- prcomp(wisc.data, scale.=TRUE)
```
```{r}
# Look at summary of results
summary(wisc.pr)
```
Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
44.27% of the variance is captured by PC1.

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
1=3 PCs.

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
7 PCs.

```{r}
biplot(wisc.pr)
```
Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
All the data points are text, so everything is illegible. There are 4 different axes which makes things confusing. This plot is impossible to read. 

```{r}
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[,1:2], col = diagnosis, 
     xlab = "PC1", ylab = "PC2")
```
```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1,3)], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```
Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
There are two main clusters, grouped by diagnosis.

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```
```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
```{r}
# Variance explained by each principal component: pve
pve <- pr.var/sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
-0.26085376
```{r}
wisc.pr$rotation[,1]
```

Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
5 PCs

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```
```{r}
data.dist <- dist(data.scaled)
```
```{r}
wisc.hclust <- hclust(data.dist, method="complete")
```

Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
About 19

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```
```{r}
table(wisc.hclust.clusters, diagnosis)
```
Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
I think that having 5 clusters may be better. They yield similar results, but when k=5, the cluster that had both "M" and "B" in the k=4 scenario is now separated into 2 clusters: one purely with "M" and the other purely with "B".
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=2)
table(wisc.hclust.clusters, diagnosis)
```
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=3)
table(wisc.hclust.clusters, diagnosis)
```
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=5)
table(wisc.hclust.clusters, diagnosis)
```
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=6)
table(wisc.hclust.clusters, diagnosis)
```
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=7)
table(wisc.hclust.clusters, diagnosis)
```
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=8)
table(wisc.hclust.clusters, diagnosis)
```
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=9)
table(wisc.hclust.clusters, diagnosis)
```
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=10)
table(wisc.hclust.clusters, diagnosis)
```
Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
I liked the "ward.D2" method the best. Although it is a bit harder to see the smaller groupings, the larger clusters are much easier to differentiate and "cut" than any of the other methods.
```{r}
wisc.hclust.single <- hclust(data.dist, method = "single")
wisc.hclust.single
plot(wisc.hclust.single)
```
```{r}
wisc.hclust.average <- hclust(data.dist, method = "average")
wisc.hclust.average
plot(wisc.hclust.average)
```
```{r}
wisc.hclust.ward.D2 <- hclust(data.dist, method = "ward.D2")
wisc.hclust.ward.D2
plot(wisc.hclust.ward.D2)
```
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method = "ward.D2")
```
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```
```{r}
g <- as.factor(grps)
levels(g)
```
```{r}
g <- relevel(g,2)
levels(g)
```
```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```
```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method = "ward.D2")
```
```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```
```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```
Q15. How well does the newly created model with four clusters separate out the two diagnoses?
Using 2 clusters, this new model seems to be separating the diagnoses better than the old model. If you set k=4 in this new model, however, the clusters are not as polarized as the old model. 
```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=4)
table(wisc.pr.hclust.clusters, diagnosis)
```